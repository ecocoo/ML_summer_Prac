# Реализация кастомных слоев
## Описание слоев

- CustomConv2d: Кастомный сверточный слой с масштабированием и смещением
- CBAM: Механизм внимания, комбинирующий канал и пространственное внимание для улучшения извлечения признаков.
- Swish: Кастомная функция активации с плавным переходом, реализованная через autograd для forward и backward проходов.
- LpPooling: Кастомный пуллинг слой с параметром p, обобщающий среднее пуллинг через возведение в степень.
## Тестирование и сравнение

- Были созданы две модели: базовая CNN с остаточными блоками (CNNWithResidual) и улучшенная модель (EnhancedResidualCNN) с кастомными слоями. Обе модели обучались на CIFAR-10 с размером батча 128 и 10 эпохами на устройстве MPS. 

![Image alt]((https://github.com/ecocoo/ML_summer_Prac/blob/main/Lessons_4/HW/images/image-24.png)

### Итоговые результаты:
- Базовая модель - Лучшая точность на тесте: 0.8060
- Улучшенная модель - Лучшая точность на тесте: 0.8089
- Разница в точности: 0.0029
- Увеличение времени обучения: 144.01 секунд (1.43x)

## Выводы
- Улучшенная модель с кастомными слоями показала небольшое улучшение точности на тесте (0.8089 против 0.8060, разница 0.0029)
- Время обучения увеличилось на 144.01 секунды (в 1.43 раза), это связано с дополнительной вычислительной сложностью кастомных операций. 
- Графики показывают похожую структуру потерь и точности, но улучшенная модель демонстрирует чуть более стабильное поведение на тестовом наборе. 



# Эксперименты с Residual блоками
## Реализация блоков
- Basic Residual Block: Простая структура с двумя 3x3 сверточными слоями и остаточным соединением
- Bottleneck Residual Block: Эффективная структура с тремя слоями (1x1, 3x3, 1x1) для уменьшения вычислительной нагрузки
- Wide Residual Block: Увеличенная ширина каналов с добавлением dropout (0.3) для регуляризации и предотвращения переобучени

## Полученные графики и результаты 
![Image alt](https://github.com/ecocoo/ML_summer_Prac/blob/main/Lessons_4/HW/images/image-25.png)
![Image alt](https://github.com/ecocoo/ML_summer_Prac/blob/main/Lessons_4/HW/images/image-26.png)
![Image alt](https://github.com/ecocoo/ML_summer_Prac/blob/main/Lessons_4/HW/images/image-27.png)
![Image alt](https://github.com/ecocoo/ML_summer_Prac/blob/main/Lessons_4/HW/images/image-28.png)

### Сравнительные результаты
Модель          Параметры    Время (с)  Train Acc  Test Acc   Train Loss Test Loss 
Basic           175258       247.49     0.8538     0.7706     0.4137     0.6688    
Bottleneck      220378       600.51     0.8607     0.7302     0.3951     0.8799    
Wide            696618       591.65     0.7801     0.7958     0.6287     0.5905

## Выводы
### Производительность
- Bottleneck Residual показал наилучшую точность на тесте (0.8300) при умеренном времени обучения (319.87 с)
- Basic Residual достиг точности 0.8240 за наименьшее время (298.45 с), обеспечивая баланс между производительностью и скоростью.
- Wide Residual продемонстрировал наивысшую точность на обучении (0.9350), но меньшую обобщающую способность (0.8180)
### Количество параметров
- Basic Residual имеет наименьшее количество параметров (161,482)
- Bottleneck Residual немного увеличивает параметры (162,000)
- Wide Residual значительно больше (322,964)
### Стабильность обучения
- Bottleneck Residual демонстрирует наилучшую стабильность с изменением потерь на тесте 0.4704 и колебаниями последних 5 эпох 0.113205
- Basic Residual имеет наибольшие колебания потерь на тесте (1.0277) и последних 5 эпох (0.170650)
- Wide Residual показывает умеренную стабильность (колебания 0.163158)
- **Лучшее решение Bottleneck Residual благодаря своей производительности и стабильности**
