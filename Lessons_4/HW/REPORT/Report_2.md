# Анализ влияния размеров ядер свертки
## Ход работы
- Были обучены нейронные сети с различными размерами ядер свертки (3x3, 5x5, 7x7 и комбинация 1x1+3x3) на наборе данных CIFAR.
-  Для каждой модели было обеспечено одинаковое количество параметров для справедливого сравнения. 
- Обучение проводилось в течение 10 эпох, при этом фиксировалось время выполнения. Также были рассчитаны рецептивные поля, и визуализированы активации первого сверточного слоя для анализа способов извлечения признаков. 
- Для реализации использовался класс **ConvKernelNet**, который определяет архитектуру сети с различными размерами ядер. 

## Полученные графики и результаты 
![3x3](images/image.png)
![5x5](images/image-1.png)
![7x7](images/image-2.png)
![1x1 + 3x3](images/image-3.png)

- Рецептивное поле модели3x3: 5
- Рецептивное поле модели5x5: 7
- Рецептивное поле модели7x7: 9
- Рецептивное поле модели1x1+3x3: 5

### Результаты:
3x3: параметры=2175626, итоговая точность на тесте=0.7209, время оубчения=842.00s
5x5: параметры=2178698, итоговая точность на тесте=0.7184, время оубчения=737.29s
7x7: параметры=2183306, итоговая точность на тесте=0.7034, время оубчения=661.69s
1x1+3x3: параметры=2192458, итоговая точность на тесте=0.7167, время оубчения=1020.48s

## Выводы
- На графике видно, что ядро 3x3 достигло наибольшей точности на тесте (0.7209), что указывает на эффективный баланс между извлечением признаков и вычислительной эффективностью. 
- Ядро 5x5 показало немного меньшую точность (0.7184), но обучалось быстрее, что может указывать на компромисс между точностью и скоростью. 
- Ядро 7x7 имело самую низкую точность (0.7034) и самое быстрое время обучения, возможно, из-за чрезмерной обобщающей способности с большими рецептивными полями. 
- Комбинация 1x1+3x3 показала среднюю точность (0.7167), но заняла больше всего времени на обучение (1020.48 с), вероятно, из-за дополнительного слоя 1x1. 
- Визуализации активаций показывают, что большие ядра захватывают более широкие признаки

# Влияние глубины CNN
## Ход работы

- Были созданы и обучены четыре модели с разной глубиной: 
- - ShallowCNN с двумя сверточными слоями 
- - MediumCNN с четырьмя слоями 
- - DeepCNN с шестью и более слоями
- - ResNet с остаточными связями. 
- Для каждой модели проводилось обучение на наборе данных CIFAR-10 в течение 10 эпох с использованием оптимизатора Adam и функции потерь CrossEntropyLoss. 
- Было измерено время обучения и финальная точность. 
- Анализировались потоки градиентов для выявления проблем vanishing/exploding gradients 
- Визуализировались карты признаков первого слоя 
- Оценивалась эффективность остаточных связей в ResNet.

## Полученные графики и результаты 
![alt text](images/image-4.png)
![alt text](images/image-5.png)
![alt text](images/image-6.png)
![alt text](images/image-7.png)
![alt text](images/image-8.png)
![alt text](images/image-9.png)
![alt text](images/image-10.png)
![alt text](images/image-11.png)

### Итоги:
ShallowCNN: точность обучения=0.7159, время обучения=104.48s, параметры=60362
MediumCNN: точность обучения=0.7701, время обучения=431.47s, параметры=150666
DeepCNN: точность обучения=0.7909, время обучения=1439.45s, параметры=613898
ResNet: точность обучения=0.7927, время обучения=352.40s, параметры=161482

## Выводы
- Анализ показывает, что увеличение глубины сети улучшает точность: 
- - ShallowCNN 0.7159 
- - MediumCNN — 0.7701
- - DeepCNN — 0.7909
- - ResNet достиг наивысшей точности (0.7927)
- Это указывает на эффективность остаточных связей, предотвращающих проблему исчезающих градиентов. 
- Время обучения значительно возрастало с глубиной
- Графики потоков градиентов демонстрируют, что в DeepCNN градиенты уменьшаются, что может указывать на vanishing gradients, тогда как в ResNet этот эффект менее выражен. 
- Карты признаков показывают, что более глубокие сети и ResNet извлекают более сложные и разнообразные признаки
